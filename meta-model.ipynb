{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7986bf-3cb2-44e3-b062-b489f94b6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from algo_pre_processing import ProjectsData\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd81bbd-8b77-4162-9a9b-5b0d75677902",
   "metadata": {},
   "source": [
    "In the first section, you will load the projects' data. After that, you can proceed to run the statistical feature experiment and the embedding-based features experiment.\n",
    "\n",
    "Since we used 2 batches of data, you will find `embedding_features1` and `embedding_features2` (or `statistical_features1` and `statistical_features2`) representing the two batches we used. However, if you are running an experiment with only one batch of data, you can choose to use only one of them. Simply assign the final dataset (`embedding_features` or `statistical_features`) as the dataset you created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79f23c-5a8b-4373-b314-d1aa6e6a033c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load projects' data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c78a42-c645-4190-88f1-753c1706b637",
   "metadata": {},
   "source": [
    "Change the following paths according to your implementation:\n",
    "- ProjectsData Object creation:\n",
    "    - `MCW_path`: the results of the label extraction part - the MCW run.\n",
    "    - `projects_path`: the path of the arff files as defined in section 2 of label extraction, you can use either `arff_source` folder or `arff_dest` folder.\n",
    "    - `features_path`: the path of the features you would like to use, for the statistical meta-features we used the output of `statistical meta features.py` and for the embedding based meta-features we used the output of the `embedding java files` folder instructions. \n",
    "    - `algo_matlab_results`: the results of the label extraction part - the MATLAB algorithms output.\n",
    "    - `to_use`: select the folders numbers that you want to use - according to the number of groups defined in `random_groups.py`.\n",
    "- Get data function: use mode for majority voting labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "393cc7cf-e323-4498-921d-2de5465b69dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading projects is finished...\n",
      "Loading projects is finished...\n"
     ]
    }
   ],
   "source": [
    "# get project's data\n",
    "projects_data = ProjectsData(MCW_path= 'data/MCW/batch1',\n",
    "                 projects_path= 'data/Projects_Arff_batch1',\n",
    "                 features_path= 'meta_model_features/embeddings/first_batch/base_buggy_features.csv',\n",
    "                 algo_matlab_results= 'data/Results_Rest_batch1',\n",
    "                 to_use=[1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    " 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50])\n",
    "embedding_features1 = projects_data.get_data(label_method='mode')\n",
    "algo_list = projects_data.get_algo_list()\n",
    "algo_scores = projects_data.get_algo_scores()\n",
    "projects_data2 = ProjectsData(MCW_path='data/MCW/batch2',\n",
    "                 projects_path='data/Projects_Arff_batch2',\n",
    "                 features_path= 'meta_model_features/embeddings/second_batch/base_buggy_features.csv',\n",
    "                 algo_matlab_results='data/Results_Rest_batch2',\n",
    "                 to_use=list(range(1, 100)))\n",
    "embedding_features2 = projects_data2.get_data(label_method='mode')\n",
    "\n",
    "embedding_features = pd.concat([embedding_features1,embedding_features2]) # embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c866b90-4e3f-41d7-b7fa-0b36aaddc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading projects is finished...\n",
      "Loading projects is finished...\n"
     ]
    }
   ],
   "source": [
    "# get project's data\n",
    "projects_data = ProjectsData(MCW_path= 'data/MCW/batch1',\n",
    "                 projects_path= 'data/Projects_Arff_batch1',\n",
    "                 features_path= 'meta_model_features/statistical_features/features_first_batch.csv',\n",
    "                 algo_matlab_results= 'data/Results_Rest_batch1',\n",
    "                            to_use=[1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    " 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50])\n",
    "statistical_features1 = projects_data.get_data(label_method='mode')\n",
    "algo_list = projects_data.get_algo_list()\n",
    "algo_scores = projects_data.get_algo_scores()\n",
    "projects_data2 = ProjectsData(MCW_path='data/MCW/batch2',\n",
    "                 projects_path='data/Projects_Arff_batch2',\n",
    "                 features_path= 'meta_model_features/statistical_features/features_second_batch.csv',\n",
    "                 algo_matlab_results='data/Results_Rest_batch2',\n",
    "                 to_use=list(range(1, 100)))\n",
    "statistical_features2 = projects_data2.get_data(label_method='mode')\n",
    "\n",
    "statistical_features = pd.concat([statistical_features1,statistical_features2]) # statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f01d08f-672c-45dc-a6d1-16a908f0910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have the same projects for embedding experiment and statistical experiment\n",
    "pv_embedding = embedding_features[['pv']]\n",
    "pv_embedding.columns = ['pv2']\n",
    "statistical_features = statistical_features.merge(pv_embedding, left_on='pv', right_on='pv2')\n",
    "statistical_features = statistical_features.drop(columns=['mode_algo'])\n",
    "statistical_features = projects_data.get_mode_value(statistical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79fa2c66-9750-4e5d-b592-07d79a0d554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_processing(algo_list, joined, y_test, preds):\n",
    "    # get the predictions of the algo + the real label\n",
    "    new_df = pd.DataFrame(y_test)\n",
    "    new_df['prediction'] = preds\n",
    "    with_res = new_df.join(joined, lsuffix=\"l\")\n",
    "    with_res = with_res[['prediction', 'best_algo'] + algo_scores]\n",
    "\n",
    "    for metric in [\"\", \"_precision\", \"_recall\", \"_accuracy\"]:\n",
    "        # get comb results\n",
    "        with_res['comb' + metric] = None\n",
    "        for algo in algo_list:\n",
    "            with_res['comb' + metric].loc[with_res['prediction'] == algo] = \\\n",
    "                with_res[with_res['prediction'] == algo][\n",
    "                    algo + metric]\n",
    "        with_res['comb' + metric] = with_res['comb' + metric].astype(float)\n",
    "\n",
    "    # get best algorithm results\n",
    "    with_res['best'] = None\n",
    "    for algo in algo_list:\n",
    "        with_res['best'].loc[with_res['best_algo'] == algo] = with_res[with_res['best_algo'] == algo][algo]\n",
    "    with_res['best'] = with_res['best'].astype(float)\n",
    "\n",
    "    # get the error for each algo\n",
    "    for algo in algo_list + ['comb']:\n",
    "        with_res[algo + \"_error\"] = (with_res['best'] - with_res[algo]).astype(float)\n",
    "    return with_res\n",
    "random_st = 14\n",
    "\n",
    "grid_dictionary = {'RF': (RandomForestClassifier(random_state=random_st), \n",
    "                          {'n_estimators': [50,100,150],\n",
    "                            'max_features': ['sqrt', 'log2'],\n",
    "                            'max_depth': [2,3],\n",
    "                            'criterion': ['gini', 'entropy', \"log_loss\"]}),\n",
    "                   'XGboost': (GradientBoostingClassifier(random_state=random_st),\n",
    "                               {\"loss\": [\"deviance\"],\n",
    "                                \"learning_rate\": [0.05,0.1],\n",
    "                                \"min_samples_split\": [0.28,0.4],\n",
    "                                \"min_samples_leaf\": [0.1,0.2],\n",
    "                                \"max_depth\": [2,3],\n",
    "                                \"max_features\": [\"sqrt\"],\n",
    "                                \"criterion\": [\"friedman_mse\",\"mse\"],\n",
    "                                \"subsample\": [0.5,0.8],\n",
    "                                \"n_estimators\": [50,100],\n",
    "                                \"warm_start\": [True, False]}),\n",
    "                  'LR': (LogisticRegression(random_state=random_st),\n",
    "                        {\n",
    "                                'penalty' : ['l1','l2'], \n",
    "                                'C'       : np.logspace(-3,3,7),\n",
    "                                'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],}),\n",
    "                  'SVM': (SVC(),\n",
    "                                {'C': [0.1, 1, 10, 100, 1000], \n",
    "                                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "                                  'kernel': ['rbf']} )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479913e-ee9f-43dc-9485-37d686f4bb38",
   "metadata": {},
   "source": [
    "## Statistical features experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95273a26-aecf-401a-8ad7-d8a0a336df50",
   "metadata": {},
   "source": [
    "1. Select the meta classifier that will be used: RF-Random Forest, XGboost, LR-Logistic Regression or SVM-Support Vector Machine.\n",
    "2. Select the number of folds for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46deb123-3135-46a8-a58f-4ab0a32208a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_CLASSIFIER = 'XGboost'\n",
    "NUM_OF_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8fc7db8-3556-42f4-90e2-31b77fa66d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_features = statistical_features[statistical_features['best_algo'] != 'Dycom']\n",
    "statistical_features = statistical_features[statistical_features['best_algo'] != 'LT']\n",
    "X = statistical_features.drop(columns=['pv'] + algo_scores + ['best_algo','mode_algo','pv2','Unnamed: 0_std','Unnamed: 0_avg', 'Unnamed: 0_min','Unnamed: 0_skew'])\n",
    "X = X.fillna(0)\n",
    "y = statistical_features['best_algo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "681a6c73-955a-464a-9e44-fe265aaeaf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCW        548\n",
       "TPTL        93\n",
       "TDS         43\n",
       "TCA_rnd     19\n",
       "Name: best_algo, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistical_features.drop_duplicates()['best_algo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfaa82e9-06f4-4082-9fd9-300c17c8cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "random_state: 14\n",
      "MCW f1:  0.2938488816491133 , MCW acc:  0.884235899117162 , MCW precision:  0.29627349681229964 , MCW recall:  0.35555693963416074\n",
      "comb f1:  0.29895197893432013 , comb acc:  0.8354232779784532 , comb precision:  0.38557822297352135 , comb recall:  0.34030821965290503\n",
      "best:  0.3241879164148779\n",
      "accuracy meta model list:  [0.8297872340425532, 0.8085106382978723, 0.8936170212765957, 0.85, 0.85]\n"
     ]
    }
   ],
   "source": [
    "acc_list_stat, f1_list = [], []\n",
    "# split to train and test + oversampling\n",
    "mean_results_stat = {'best':[],'MCW': [], 'MCW_precision': [], 'MCW_recall': [], 'MCW_accuracy': [], 'comb': [], 'comb_precision': [], 'comb_recall': [],\n",
    " 'comb_accuracy': [], 'TPTL': [], 'TPTL_precision': [], 'TPTL_recall': [], 'TPTL_accuracy': [], 'TCA_rnd': [], 'TCA_rnd_precision': [],\n",
    " 'TCA_rnd_recall': [], 'TCA_rnd_accuracy': [], 'LT': [], 'LT_precision': [], 'LT_recall': [], 'LT_accuracy': [], 'Dycom': [], 'Dycom_precision': [],\n",
    " 'Dycom_recall': [], 'Dycom_accuracy': [], 'TDS': [], 'TDS_precision': [], 'TDS_recall': [], 'TDS_accuracy': []}\n",
    "\n",
    "error_results = {'MCW': [], 'comb': [], 'TPTL': [], 'TCA_rnd': [], 'LT': [], 'Dycom': [], 'TDS': []}\n",
    "ss = KFold(n_splits=NUM_OF_FOLDS,shuffle=True,random_state=14)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in ss.split(X,y):\n",
    "    fold += 1\n",
    "    scaler = StandardScaler()\n",
    "    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "    sm = SMOTE(random_state=random_st, k_neighbors=2)\n",
    "    X_samp, y_samp = sm.fit_resample(x_train, y_train)\n",
    "    print(\"Oversampling is finished....\")\n",
    "    # building the model\n",
    "    rfc = grid_dictionary[META_CLASSIFIER][0]\n",
    "    param_grid = grid_dictionary[META_CLASSIFIER][1]\n",
    "\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n",
    "    CV_rfc.fit(X_samp, y_samp)\n",
    "\n",
    "    preds = CV_rfc.best_estimator_.predict(x_test)\n",
    "    print(CV_rfc.best_estimator_)\n",
    "    acc_list_stat.append(accuracy_score(y_test, preds))\n",
    "    f1_list.append(f1_score(y_test, preds, average='micro'))\n",
    "\n",
    "    ####### RESULTS ANALYSIS #######\n",
    "    with_res = results_processing(algo_list, statistical_features, y_test, preds)\n",
    "    \n",
    "    for algo in ['MCW', 'comb', 'TPTL', 'TCA_rnd', 'LT', 'Dycom', 'TDS']:\n",
    "        mean_results_stat[algo].append(np.mean(with_res[algo]))\n",
    "        mean_results_stat[algo + \"_precision\"].append(np.mean(with_res[algo + \"_precision\"]))\n",
    "        mean_results_stat[algo + \"_recall\"].append(np.mean(with_res[algo + \"_recall\"]))\n",
    "        mean_results_stat[algo + \"_accuracy\"].append(np.mean(with_res[algo + \"_accuracy\"]))\n",
    "        error_results[algo].append(np.mean(with_res[algo + \"_error\"]))\n",
    "    mean_results_stat['best'].append(np.mean(with_res['best']))\n",
    "    with_res.to_csv(f\"Results_meta_model/current_results/statistical_result_{fold}.csv\")\n",
    "    \n",
    "print('random_state:', random_st)\n",
    "print(\"MCW f1: \", np.mean(mean_results_stat['MCW']), \", MCW acc: \", np.mean(mean_results_stat['MCW_accuracy']),\n",
    "      \", MCW precision: \", np.mean(mean_results_stat['MCW_precision']), \", MCW recall: \",\n",
    "      np.mean(mean_results_stat['MCW_recall']))\n",
    "print(\"comb f1: \", np.mean(mean_results_stat['comb']), \", comb acc: \", np.mean(mean_results_stat['comb_accuracy']),\n",
    "      \", comb precision: \", np.mean(mean_results_stat['comb_precision']), \", comb recall: \",\n",
    "      np.mean(mean_results_stat['comb_recall']))\n",
    "print(\"best: \", np.mean(mean_results_stat['best']))\n",
    "print(\"accuracy meta model list: \" , acc_list_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95306828-fe57-4cd5-986c-99a2fcb6302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>MCW</th>\n",
       "      <th>MCW_precision</th>\n",
       "      <th>MCW_recall</th>\n",
       "      <th>MCW_accuracy</th>\n",
       "      <th>comb</th>\n",
       "      <th>comb_precision</th>\n",
       "      <th>comb_recall</th>\n",
       "      <th>comb_accuracy</th>\n",
       "      <th>TPTL</th>\n",
       "      <th>...</th>\n",
       "      <th>LT_recall</th>\n",
       "      <th>LT_accuracy</th>\n",
       "      <th>Dycom</th>\n",
       "      <th>Dycom_precision</th>\n",
       "      <th>Dycom_recall</th>\n",
       "      <th>Dycom_accuracy</th>\n",
       "      <th>TDS</th>\n",
       "      <th>TDS_precision</th>\n",
       "      <th>TDS_recall</th>\n",
       "      <th>TDS_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.330294</td>\n",
       "      <td>0.307057</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.384835</td>\n",
       "      <td>0.892901</td>\n",
       "      <td>0.300967</td>\n",
       "      <td>0.381728</td>\n",
       "      <td>0.360887</td>\n",
       "      <td>0.845211</td>\n",
       "      <td>0.170108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162853</td>\n",
       "      <td>0.904410</td>\n",
       "      <td>0.108718</td>\n",
       "      <td>0.281733</td>\n",
       "      <td>0.123286</td>\n",
       "      <td>0.738009</td>\n",
       "      <td>0.138784</td>\n",
       "      <td>0.548405</td>\n",
       "      <td>0.091758</td>\n",
       "      <td>0.599550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.346266</td>\n",
       "      <td>0.309555</td>\n",
       "      <td>0.326753</td>\n",
       "      <td>0.353262</td>\n",
       "      <td>0.883733</td>\n",
       "      <td>0.312707</td>\n",
       "      <td>0.419149</td>\n",
       "      <td>0.332698</td>\n",
       "      <td>0.839235</td>\n",
       "      <td>0.186643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145573</td>\n",
       "      <td>0.891910</td>\n",
       "      <td>0.119757</td>\n",
       "      <td>0.282359</td>\n",
       "      <td>0.124523</td>\n",
       "      <td>0.730365</td>\n",
       "      <td>0.165816</td>\n",
       "      <td>0.545690</td>\n",
       "      <td>0.117522</td>\n",
       "      <td>0.623025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.323066</td>\n",
       "      <td>0.295366</td>\n",
       "      <td>0.285046</td>\n",
       "      <td>0.361477</td>\n",
       "      <td>0.876504</td>\n",
       "      <td>0.308079</td>\n",
       "      <td>0.387107</td>\n",
       "      <td>0.359182</td>\n",
       "      <td>0.820011</td>\n",
       "      <td>0.181867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158895</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>0.115055</td>\n",
       "      <td>0.262673</td>\n",
       "      <td>0.133048</td>\n",
       "      <td>0.745068</td>\n",
       "      <td>0.153868</td>\n",
       "      <td>0.518993</td>\n",
       "      <td>0.113717</td>\n",
       "      <td>0.619589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.295705</td>\n",
       "      <td>0.260186</td>\n",
       "      <td>0.269828</td>\n",
       "      <td>0.320819</td>\n",
       "      <td>0.885216</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>0.364903</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.844962</td>\n",
       "      <td>0.169433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>0.906480</td>\n",
       "      <td>0.106914</td>\n",
       "      <td>0.259459</td>\n",
       "      <td>0.115239</td>\n",
       "      <td>0.762344</td>\n",
       "      <td>0.154110</td>\n",
       "      <td>0.555324</td>\n",
       "      <td>0.103495</td>\n",
       "      <td>0.650073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325609</td>\n",
       "      <td>0.297082</td>\n",
       "      <td>0.291640</td>\n",
       "      <td>0.357392</td>\n",
       "      <td>0.882825</td>\n",
       "      <td>0.302692</td>\n",
       "      <td>0.375003</td>\n",
       "      <td>0.346093</td>\n",
       "      <td>0.827697</td>\n",
       "      <td>0.166095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164099</td>\n",
       "      <td>0.899776</td>\n",
       "      <td>0.098842</td>\n",
       "      <td>0.249429</td>\n",
       "      <td>0.120544</td>\n",
       "      <td>0.738947</td>\n",
       "      <td>0.146722</td>\n",
       "      <td>0.567562</td>\n",
       "      <td>0.099145</td>\n",
       "      <td>0.608196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       best       MCW  MCW_precision  MCW_recall  MCW_accuracy      comb  \\\n",
       "0  0.330294  0.307057       0.308100    0.384835      0.892901  0.300967   \n",
       "1  0.346266  0.309555       0.326753    0.353262      0.883733  0.312707   \n",
       "2  0.323066  0.295366       0.285046    0.361477      0.876504  0.308079   \n",
       "3  0.295705  0.260186       0.269828    0.320819      0.885216  0.270314   \n",
       "4  0.325609  0.297082       0.291640    0.357392      0.882825  0.302692   \n",
       "\n",
       "   comb_precision  comb_recall  comb_accuracy      TPTL  ...  LT_recall  \\\n",
       "0        0.381728     0.360887       0.845211  0.170108  ...   0.162853   \n",
       "1        0.419149     0.332698       0.839235  0.186643  ...   0.145573   \n",
       "2        0.387107     0.359182       0.820011  0.181867  ...   0.158895   \n",
       "3        0.364903     0.302682       0.844962  0.169433  ...   0.165044   \n",
       "4        0.375003     0.346093       0.827697  0.166095  ...   0.164099   \n",
       "\n",
       "   LT_accuracy     Dycom  Dycom_precision  Dycom_recall  Dycom_accuracy  \\\n",
       "0     0.904410  0.108718         0.281733      0.123286        0.738009   \n",
       "1     0.891910  0.119757         0.282359      0.124523        0.730365   \n",
       "2     0.894400  0.115055         0.262673      0.133048        0.745068   \n",
       "3     0.906480  0.106914         0.259459      0.115239        0.762344   \n",
       "4     0.899776  0.098842         0.249429      0.120544        0.738947   \n",
       "\n",
       "        TDS  TDS_precision  TDS_recall  TDS_accuracy  \n",
       "0  0.138784       0.548405    0.091758      0.599550  \n",
       "1  0.165816       0.545690    0.117522      0.623025  \n",
       "2  0.153868       0.518993    0.113717      0.619589  \n",
       "3  0.154110       0.555324    0.103495      0.650073  \n",
       "4  0.146722       0.567562    0.099145      0.608196  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('max_columns',30)\n",
    "pd.DataFrame(mean_results_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78ba5c4-401e-4b42-b6e6-da0921211de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_unioned = []\n",
    "for i in range(1,6):\n",
    "    temp = pd.read_csv(f'Results_meta_model/current_results/statistical_result_{i}.csv')\n",
    "    res_unioned.append(temp)\n",
    "res_pd = pd.concat(res_unioned)\n",
    "res_pd.to_csv('statistical_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea86656-bf57-4428-a420-78149e6f9d87",
   "metadata": {},
   "source": [
    "## Embedding based features experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b130164-496c-41f7-a454-4a2bf611b734",
   "metadata": {},
   "source": [
    "1. Select the meta classifier that will be used: RF-Random Forest, XGboost, LR-Logistic Regression or SVM-Support Vector Machine.\n",
    "2. Select the number of folds for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d0b9993-71c9-4663-9913-613392e1a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_CLASSIFIER = 'XGboost'\n",
    "NUM_OF_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c27e0f-26e4-4230-8bde-690a6315c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features = embedding_features.drop(columns=['mode_algo'])\n",
    "embedding_features = projects_data.get_mode_value(embedding_features)\n",
    "embedding_features = embedding_features[embedding_features['best_algo'] != 'Dycom']\n",
    "embedding_features = embedding_features[embedding_features['best_algo'] != 'LT']\n",
    "X = embedding_features.drop(columns=['pv'] + algo_scores + ['best_algo','mode_algo'])\n",
    "X = X.fillna(0)\n",
    "y = embedding_features['best_algo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b7ae76-da9c-423e-a60b-c9d684656684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCW        548\n",
       "TPTL        93\n",
       "TDS         43\n",
       "TCA_rnd     19\n",
       "Name: best_algo, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_features.drop_duplicates()['best_algo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89adccf2-d8ec-4566-a16f-bc0f03c87ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "Oversampling is finished....\n",
      "GradientBoostingClassifier(loss='deviance', max_features='sqrt',\n",
      "                           min_samples_leaf=0.1, min_samples_split=0.28,\n",
      "                           random_state=14, subsample=0.8, warm_start=True)\n",
      "random_state: 14\n",
      "MCW f1:  0.2938510411547034 , MCW acc:  0.8842293506160919 , MCW precision:  0.29626380249841533 , MCW recall:  0.3555660488742657\n",
      "comb f1:  0.31016998343769064 , comb acc:  0.8456790022686962 , comb precision:  0.3760124262850176 , comb recall:  0.35809697468337853\n",
      "best:  0.32419226266699763\n",
      "accuracy meta model list:  [0.9290780141843972, 0.8794326241134752, 0.9432624113475178, 0.8785714285714286, 0.8857142857142857]\n"
     ]
    }
   ],
   "source": [
    "acc_list_embd, f1_list = [], []\n",
    "# split to train and test + oversampling\n",
    "mean_results_embd = {'best':[],'MCW': [], 'MCW_precision': [], 'MCW_recall': [], 'MCW_accuracy': [], 'comb': [], 'comb_precision': [], 'comb_recall': [],\n",
    " 'comb_accuracy': [], 'TPTL': [], 'TPTL_precision': [], 'TPTL_recall': [], 'TPTL_accuracy': [], 'TCA_rnd': [], 'TCA_rnd_precision': [],\n",
    " 'TCA_rnd_recall': [], 'TCA_rnd_accuracy': [], 'LT': [], 'LT_precision': [], 'LT_recall': [], 'LT_accuracy': [], 'Dycom': [], 'Dycom_precision': [],\n",
    " 'Dycom_recall': [], 'Dycom_accuracy': [], 'TDS': [], 'TDS_precision': [], 'TDS_recall': [], 'TDS_accuracy': []}\n",
    "\n",
    "error_results = {'MCW': [], 'comb': [], 'TPTL': [], 'TCA_rnd': [], 'LT': [], 'Dycom': [], 'TDS': []}\n",
    "ss = KFold(n_splits=NUM_OF_FOLDS,shuffle=True,random_state=14)\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in ss.split(X,y):\n",
    "    fold += 1\n",
    "    scaler = StandardScaler()\n",
    "    x_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    x_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "    sm = SMOTE(random_state=random_st, k_neighbors=2)\n",
    "    X_samp, y_samp = sm.fit_resample(x_train, y_train)\n",
    "    print(\"Oversampling is finished....\")\n",
    "    # building the model\n",
    "    rfc = grid_dictionary[META_CLASSIFIER][0]\n",
    "    param_grid = grid_dictionary[META_CLASSIFIER][1]\n",
    "\n",
    "    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n",
    "    CV_rfc.fit(X_samp, y_samp)\n",
    "\n",
    "    preds = CV_rfc.best_estimator_.predict(x_test)\n",
    "    print(CV_rfc.best_estimator_)\n",
    "    acc_list_embd.append(accuracy_score(y_test, preds))\n",
    "    f1_list.append(f1_score(y_test, preds, average='micro'))\n",
    "\n",
    "    ####### RESULTS ANALYSIS #######\n",
    "    with_res = results_processing(algo_list, embedding_features, y_test, preds)\n",
    "    \n",
    "    for algo in ['MCW', 'comb', 'TPTL', 'TCA_rnd', 'LT', 'Dycom', 'TDS']:\n",
    "        mean_results_embd[algo].append(np.mean(with_res[algo]))\n",
    "        mean_results_embd[algo + \"_precision\"].append(np.mean(with_res[algo + \"_precision\"]))\n",
    "        mean_results_embd[algo + \"_recall\"].append(np.mean(with_res[algo + \"_recall\"]))\n",
    "        mean_results_embd[algo + \"_accuracy\"].append(np.mean(with_res[algo + \"_accuracy\"]))\n",
    "        error_results[algo].append(np.mean(with_res[algo + \"_error\"]))\n",
    "    mean_results_embd['best'].append(np.mean(with_res['best']))\n",
    "    # results_all = with_res.describe()\n",
    "    with_res.to_csv(f\"Results_meta_model/current_results/embedding_result_{fold}.csv\")\n",
    "    \n",
    "print('random_state:', random_st)\n",
    "print(\"MCW f1: \", np.mean(mean_results_embd['MCW']), \", MCW acc: \", np.mean(mean_results_embd['MCW_accuracy']),\n",
    "      \", MCW precision: \", np.mean(mean_results_embd['MCW_precision']), \", MCW recall: \",\n",
    "      np.mean(mean_results_embd['MCW_recall']))\n",
    "print(\"comb f1: \", np.mean(mean_results_embd['comb']), \", comb acc: \", np.mean(mean_results_embd['comb_accuracy']),\n",
    "      \", comb precision: \", np.mean(mean_results_embd['comb_precision']), \", comb recall: \",\n",
    "      np.mean(mean_results_embd['comb_recall']))\n",
    "print(\"best: \", np.mean(mean_results_embd['best']))\n",
    "print(\"accuracy meta model list: \" , acc_list_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e5f78a5-c731-4357-be6c-41a46aaf884b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>MCW</th>\n",
       "      <th>MCW_precision</th>\n",
       "      <th>MCW_recall</th>\n",
       "      <th>MCW_accuracy</th>\n",
       "      <th>comb</th>\n",
       "      <th>comb_precision</th>\n",
       "      <th>comb_recall</th>\n",
       "      <th>comb_accuracy</th>\n",
       "      <th>TPTL</th>\n",
       "      <th>...</th>\n",
       "      <th>LT_recall</th>\n",
       "      <th>LT_accuracy</th>\n",
       "      <th>Dycom</th>\n",
       "      <th>Dycom_precision</th>\n",
       "      <th>Dycom_recall</th>\n",
       "      <th>Dycom_accuracy</th>\n",
       "      <th>TDS</th>\n",
       "      <th>TDS_precision</th>\n",
       "      <th>TDS_recall</th>\n",
       "      <th>TDS_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.308862</td>\n",
       "      <td>0.284735</td>\n",
       "      <td>0.273869</td>\n",
       "      <td>0.366862</td>\n",
       "      <td>0.891715</td>\n",
       "      <td>0.298428</td>\n",
       "      <td>0.342619</td>\n",
       "      <td>0.368449</td>\n",
       "      <td>0.856584</td>\n",
       "      <td>0.164394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150174</td>\n",
       "      <td>0.901518</td>\n",
       "      <td>0.112992</td>\n",
       "      <td>0.270057</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>0.748458</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.543317</td>\n",
       "      <td>0.099247</td>\n",
       "      <td>0.616961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.359682</td>\n",
       "      <td>0.329853</td>\n",
       "      <td>0.345099</td>\n",
       "      <td>0.381157</td>\n",
       "      <td>0.888389</td>\n",
       "      <td>0.348517</td>\n",
       "      <td>0.421455</td>\n",
       "      <td>0.383291</td>\n",
       "      <td>0.854951</td>\n",
       "      <td>0.181445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159233</td>\n",
       "      <td>0.892166</td>\n",
       "      <td>0.114811</td>\n",
       "      <td>0.279202</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.731632</td>\n",
       "      <td>0.150781</td>\n",
       "      <td>0.546699</td>\n",
       "      <td>0.107920</td>\n",
       "      <td>0.612172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.328039</td>\n",
       "      <td>0.295878</td>\n",
       "      <td>0.307718</td>\n",
       "      <td>0.345178</td>\n",
       "      <td>0.877619</td>\n",
       "      <td>0.320451</td>\n",
       "      <td>0.408509</td>\n",
       "      <td>0.352427</td>\n",
       "      <td>0.833854</td>\n",
       "      <td>0.184204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195457</td>\n",
       "      <td>0.898608</td>\n",
       "      <td>0.117727</td>\n",
       "      <td>0.275406</td>\n",
       "      <td>0.130782</td>\n",
       "      <td>0.738622</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>0.528190</td>\n",
       "      <td>0.106986</td>\n",
       "      <td>0.624451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.303277</td>\n",
       "      <td>0.267051</td>\n",
       "      <td>0.252118</td>\n",
       "      <td>0.344341</td>\n",
       "      <td>0.882824</td>\n",
       "      <td>0.281708</td>\n",
       "      <td>0.322132</td>\n",
       "      <td>0.345987</td>\n",
       "      <td>0.844795</td>\n",
       "      <td>0.171455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151402</td>\n",
       "      <td>0.904194</td>\n",
       "      <td>0.102899</td>\n",
       "      <td>0.259948</td>\n",
       "      <td>0.109844</td>\n",
       "      <td>0.738786</td>\n",
       "      <td>0.156970</td>\n",
       "      <td>0.542270</td>\n",
       "      <td>0.107129</td>\n",
       "      <td>0.634979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321101</td>\n",
       "      <td>0.291739</td>\n",
       "      <td>0.302515</td>\n",
       "      <td>0.340292</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>0.301746</td>\n",
       "      <td>0.385347</td>\n",
       "      <td>0.340332</td>\n",
       "      <td>0.838211</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139748</td>\n",
       "      <td>0.900435</td>\n",
       "      <td>0.100799</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.132890</td>\n",
       "      <td>0.757159</td>\n",
       "      <td>0.155315</td>\n",
       "      <td>0.575606</td>\n",
       "      <td>0.104425</td>\n",
       "      <td>0.611512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       best       MCW  MCW_precision  MCW_recall  MCW_accuracy      comb  \\\n",
       "0  0.308862  0.284735       0.273869    0.366862      0.891715  0.298428   \n",
       "1  0.359682  0.329853       0.345099    0.381157      0.888389  0.348517   \n",
       "2  0.328039  0.295878       0.307718    0.345178      0.877619  0.320451   \n",
       "3  0.303277  0.267051       0.252118    0.344341      0.882824  0.281708   \n",
       "4  0.321101  0.291739       0.302515    0.340292      0.880600  0.301746   \n",
       "\n",
       "   comb_precision  comb_recall  comb_accuracy      TPTL  ...  LT_recall  \\\n",
       "0        0.342619     0.368449       0.856584  0.164394  ...   0.150174   \n",
       "1        0.421455     0.383291       0.854951  0.181445  ...   0.159233   \n",
       "2        0.408509     0.352427       0.833854  0.184204  ...   0.195457   \n",
       "3        0.322132     0.345987       0.844795  0.171455  ...   0.151402   \n",
       "4        0.385347     0.340332       0.838211  0.172734  ...   0.139748   \n",
       "\n",
       "   LT_accuracy     Dycom  Dycom_precision  Dycom_recall  Dycom_accuracy  \\\n",
       "0     0.901518  0.112992         0.270057      0.123670        0.748458   \n",
       "1     0.892166  0.114811         0.279202      0.119629        0.731632   \n",
       "2     0.898608  0.117727         0.275406      0.130782        0.738622   \n",
       "3     0.904194  0.102899         0.259948      0.109844        0.738786   \n",
       "4     0.900435  0.100799         0.250994      0.132890        0.757159   \n",
       "\n",
       "        TDS  TDS_precision  TDS_recall  TDS_accuracy  \n",
       "0  0.143098       0.543317    0.099247      0.616961  \n",
       "1  0.150781       0.546699    0.107920      0.612172  \n",
       "2  0.153226       0.528190    0.106986      0.624451  \n",
       "3  0.156970       0.542270    0.107129      0.634979  \n",
       "4  0.155315       0.575606    0.104425      0.611512  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(mean_results_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf2578e-3a80-476d-9d11-5029bbbe78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_unioned = []\n",
    "for i in range(1,6):\n",
    "    temp = pd.read_csv(f'Results_meta_model/current_results/embedding_result_{i}.csv')\n",
    "    res_unioned.append(temp)\n",
    "res_pd = pd.concat(res_unioned)\n",
    "res_pd.to_csv('embedding_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6beaf0d-cddf-4bf9-bfce-90dd2d356cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
